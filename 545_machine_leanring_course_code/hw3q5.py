# -*- coding: utf-8 -*-
"""HW3Q5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oeIxPaew6N1uz2ShF6EzzjlCIZJsGyEo
"""

import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)

xtrain = np.load("housing_train_features.npy")
xtest = np.load("housing_test_features.npy")
ytrain = np.load("housing_train_labels.npy")
ytest = np.load("housing_test_labels.npy")

feature_names = np.load("housing_feature_names.npy", allow_pickle=True)
print("First feature name: ", feature_names[0])
print("Lot frontage for first train sample:", xtrain[0,0])

d, n = xtrain.shape

def normalization(input_mat):
  row_means = []
  row_vars = []
  for i in range(xtrain.shape[0]):
    sample_mean = np.mean(xtrain[i,:])
    sample_var = np.sum((xtrain[i, :] - sample_mean)**2) / xtrain.shape[1]
    row_means.append(sample_mean)
    row_vars.append(np.sqrt(sample_var))

    input_mat[i, :] = (input_mat[i, :] - sample_mean) / np.sqrt(sample_var) 

  return input_mat, row_means, row_vars

xtrain, row_means, row_vars = normalization(xtrain)
row_means = np.array(row_means)
row_means = row_means.reshape(-1, 1)

w0 = np.sum(ytrain) / n 
w0

def generate_what(lam, w, w0, xtrain, ytrain, ):
  d, n = xtrain.shape
  store_vals = np.ndarray((d, 0))
  for i in range(xtrain.shape[0]):
    wi = np.ones(d)
    wi[i] = 0
    innerpart = ytrain - w0 - np.dot((w.T * wi), xtrain)
    ci = 2 * np.dot(innerpart, xtrain[i,:])
    ai = 2 * np.dot(xtrain[i,:].T, xtrain[i,:])
    if ci < - lam:
      w[i,0] = ((ci + lam) / ai)
    elif ci > lam:
      w[i,0] = ((ci - lam) / ai)
    else:
      w[i, 0] = 0
    store_vals = np.concatenate((store_vals, w), axis = 1)
  return store_vals, w

# Repeat the function above 50 times with initial w of all ones
w = np.ones((d, 1))
results = np.ndarray((d, 0))
for i in range(50):
  store_vals, w = generate_what(100, w, w0, xtrain, ytrain)
  results = np.concatenate((results, store_vals), axis = 1)

# The converged weights are shown below
print(results[:,-1])

# Change the mean of xtest data set
for i in range(xtest.shape[0]):
  xtest[i, :] = (xtest[i, :] - row_means[i]) / row_vars[i]

converged_w = results[:,-1]
y_hat = w0 + np.dot(xtest.T, converged_w)

# The final test MSE is shown below
total_vals = 0
for idx in range(ytest.shape[0]):
  total_vals = total_vals + (ytest[idx] - y_hat[idx]) ** 2
mse = total_vals / ytest.shape[0]
print(mse)


# The trajectories over iteration are shown below
for i in range(results.shape[0]):
  plt.plot(results[i,:])
plt.show()



# The learning curve is shown below 
all_vals = []
for i in range(results.shape[1]):
  total_vals = 0
  what = results[:,i]
  what = what.reshape(-1, 1)
  y_hat = w0 + np.dot(xtrain.T, what)
  for idx in range(ytrain.shape[0]):
    total_vals = total_vals + (ytrain[idx] - y_hat[idx]) ** 2
  
  total_vals = total_vals / ytrain.shape[0]
  all_vals.append(total_vals)

iter = np.linspace(0, 2899, 2900)

plt.plot(iter, all_vals)
plt.xlabel("# of iteration")
plt.ylabel("MSE of training dataset")

