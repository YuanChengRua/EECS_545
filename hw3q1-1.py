# -*- coding: utf-8 -*-
"""HW3Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HeMW-3Ut-0VvGm2sM2K_Iv_gG_fFfRdo
"""

import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)

xtrain = np.load("housing_train_features.npy")
xtest = np.load("housing_test_features.npy")
ytrain = np.load("housing_train_labels.npy")
ytest = np.load("housing_test_labels.npy")

feature_names = np.load("housing_feature_names.npy", allow_pickle=True)
print("First feature name: ", feature_names[0])
print("Lot frontage for first train sample:", xtrain[0,0])

d, n = xtrain.shape

def normalization(input_mat):
  row_means = []
  row_vars = []
  for i in range(xtrain.shape[0]):
    sample_mean = np.mean(xtrain[i,:])
    sample_var = np.sum((xtrain[i, :] - sample_mean)**2) / xtrain.shape[1]
    row_means.append(sample_mean)
    row_vars.append(np.sqrt(sample_var))

    input_mat[i, :] = (input_mat[i, :] - sample_mean) / np.sqrt(sample_var) 

  return input_mat, row_means, row_vars

xtrain, row_means, row_vars = normalization(xtrain)
row_means = np.array(row_means)
row_means = row_means.reshape(-1, 1)

plt.plot(list(range(len(row_means))), row_means)  # Plot the the vector means and the vector of variances 
plt.figure()
plt.plot(list(range(len(row_vars))), row_vars)

# The value of the first 5 w_hat is shown below, as well as w0

def generate_what(xtrain, ytrain, lam):
  a = np.ones((1, xtrain.shape[1]))
  # xtrain_with_ones = np.vstack([a, xtrain])
  ytrain = ytrain.reshape(-1,1)
  # ytrain = ytrain - np.mean(ytrain)

  inv_part = np.linalg.inv(np.matmul(xtrain, xtrain.T) + lam * np.identity(xtrain.shape[0]))
  what = np.dot(np.matmul(inv_part, xtrain), ytrain)
  
  return what

what = generate_what(xtrain, ytrain, 100)
print(what[:5])
def generate_w0(row_means, what, ytrain):
  w0 = np.mean(ytrain)
  return w0 

w0 = generate_w0(row_means, what, ytrain)
print(w0)


# We can see that the first 5 Ws are W1 = 3.02355147, W2 = 4.58605397, W3 = 2.38090826, W4 = 0.22324962, W5 = -0.4434401 and W0 = 181.678874

store_vals = np.ndarray((58, 0))
for lam in np.linspace(500, 100000, 200):
  what = generate_what(xtrain, ytrain, lam)
  # plt.plot(what.reshape(58,1))
  store_vals = np.concatenate((store_vals, what), axis = 1)

for i in range(store_vals.shape[0]):
  plt.plot(store_vals[i,:])
plt.show()

# The trajectories of the d ridge coefficients are shown below

MSE_train = []
for lam in np.linspace(500, 100000, 200):
  what = generate_what(xtrain, ytrain, lam)
  w0 = generate_w0(xtrain, what, ytrain)

  mse_train = 0
  for j in range(n):
    mse_train = mse_train + (np.dot(what.T, xtrain[:, j]) + w0 - ytrain[j]) ** 2
  mse_train = mse_train / n
  MSE_train.append(mse_train[0])


lamb = np.linspace(500, 100000, 200)
# The training data's MSE is plotted below as lambda varies over the range 
plt.plot(lamb, MSE_train)
print(MSE_train[-1])
print(min(MSE_train))

MSE_train = []

for lam in np.linspace(1, 100, 200):
  what = generate_what(xtrain, ytrain, lam)
  w0 = generate_w0(xtrain, what, ytrain)

  mse_train = 0
  for j in range(n):
    mse_train = mse_train + (np.dot(what.T, xtrain[:, j]) + w0 - ytrain[j]) ** 2

  mse_train = mse_train / n
  MSE_train.append(mse_train[0])

lamb = np.linspace(1, 100, 200)

plt.plot(lamb, MSE_train)
min_idx_train = MSE_train.index(min(MSE_train))

for i in range(xtest.shape[0]):
  xtest[i, :] = (xtest[i, :] - row_means[i]) / row_vars[i] 

MSE_test_d = []

for lam in np.linspace(0.5, 100, 200):
  what_d = generate_what(xtrain, ytrain, lam)
  w0 = generate_w0(xtrain, what, ytrain)

  mse_test_d = 0
  for j in range(xtest.shape[1]):
    mse_test_d = mse_test_d + (np.dot(what_d.T, xtest[:,j]) + w0 - ytest[j]) ** 2
  
  mse_test_d = mse_test_d / xtest.shape[1]

  MSE_test_d.append(mse_test_d)

print(min(MSE_test_d))
lamb = np.linspace(1, 100, 200)
plt.plot(lamb, MSE_test_d)

min_idx = MSE_test_d.index(min(MSE_test_d))
print(min_idx)

# The corresponding lambda of the minimum test MSE is 87.56
print(lamb[min_idx])

# The corresponding lambda of the minimum train MSE is 0.5
print(lamb[min_idx_train])

# According to the observation, the MSE of training data is generally increasing with respect to the value of lambda
# However, the MSE of testing data is decreasing with respect to the change of lambda.
# Also the test MSE is generally higher than the training data's MSE