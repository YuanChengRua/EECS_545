{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW2.1_yuacheng",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjJMpc4ocIBV"
      },
      "source": [
        "# EECS 504 Homework 2\n",
        "\n",
        "## Problem 1: Implement neural network layers from scratch\n",
        "\n",
        "In this assignment, you'll learn and practice implementing forward and backward pass for different layers used in neural networks. You'll implement two different classifier models - MultiLayer Perceptron and CNN classifier, and train these on CIFAR-10 and MNIST datasets respectively.\n",
        "\n",
        "### Getting Started\n",
        "Make your own copy of this notebook using **File -> Save a copy in Drive**, or if you have iPython installed locally, **File -> Download .ipynb** to do the exercises locally.\n",
        "\n",
        "### Submitting\n",
        "When you're ready to submit this problem, first run all of the code to ensure your outputs are included in the submission. Save the `.ipynb` file and submit it to Canvas as `HW2.1_<uniqname>.ipynb`, substituting your uniqname. Be sure your name, uniqname, and UMID are correct in the form fields below. \n",
        "\n",
        "**We would also want you to include a pdf version of your final notebook with outputs in your canvas submission, alongwith .ipynb file. You can follow these steps to save a pdf version. After completing your code and running all cells to include output, you can go to File -> Print -> Save as pdf and choose Landscape mode in Colab. Naming convention is same as that for .ipynb file.**\n",
        "\n",
        "\n",
        "If you want to preview how it will look when we grade it, we'll run a command similar to the one below:\n",
        "\n",
        "`ipython nbconvert --to html HW2.1_<uniqname>.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaCvEC88cNjx"
      },
      "source": [
        "#@title Grading information\n",
        "Name = 'Yuan Cheng' #@param {type: 'string'}\n",
        "Uniqname = 'yuacheng' #@param {type: 'string'}\n",
        "UMID = '06358288' #@param {type: 'string'}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Xxj41ycS6M"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules and download all the files you'll need.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_p0jH-yb5f-"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "import math\n",
        "from keras.datasets import mnist\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyDGzG8tca95"
      },
      "source": [
        "# Problem 1.1 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "You'll implement the following network architecture: \n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The output of the network are the scores for each class. And we will train this network with a cross entropy loss function. In our implementation here, softmax_loss() function will compute softmax activation for the output of last fully-connected layer and calculate cross-entropy loss.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-qFO7B8cjyv"
      },
      "source": [
        "# Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8GCN1WDccO4"
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    out_part1 = np.dot(x, w)\n",
        "    out = out_part1 + b\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    \n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    dx = np.dot(dout, w.T)\n",
        "    dw = np.dot(x.T, dout)\n",
        "    db = np.sum(dout, axis=0)\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    \n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    out = np.maximum(0,x)\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "     \n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    x = cache\n",
        "    dout[x <= 0] = 0\n",
        "    dx = dout\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the softmax activation of the input and calculates the \n",
        "    cross-entropy loss. \n",
        "    Also computes the gradient of the loss with respect to the input. \n",
        "    \n",
        "    For testing, input y is None and only compute the softmax activation of \n",
        "    the input.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) - this should be the output of the last \n",
        "    fully connected layer\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C or None\n",
        "\n",
        "    if y is None:\n",
        "      Returns:\n",
        "      - scores : Vector of class scores, of shape (N,C) - this should be \n",
        "      softmax activation of the input\n",
        "    else:\n",
        "      Returns a tuple of:\n",
        "      - loss: Scalar giving the loss\n",
        "      - dx: Gradient of the loss with respect to x\n",
        "    \n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                            #\n",
        "    ###########################################################################\n",
        "    exp_scores = np.exp(x)\n",
        "    sum_exp_scores = np.sum(exp_scores, axis = 1, keepdims = True)\n",
        "    log_values = np.log(sum_exp_scores)\n",
        "    loss = - np.sum(log_values)\n",
        "    probs = exp_scores / sum_exp_scores\n",
        "    loss = loss / x.shape[0]\n",
        "    dx = exp_scores\n",
        "    for i in range(x.shape[0]):\n",
        "      dx[i, y[i]] -= 1\n",
        "    dx = dx / x.shape[0]\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    if y is None:\n",
        "      return scores\n",
        "    else:\n",
        "      return loss, dx\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm_2JxQkc6jU"
      },
      "source": [
        "# MLP Classifier\n",
        "\n",
        "In this problem, implement MLP classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQVqKQ3c3TM"
      },
      "source": [
        "class MLPClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network that uses a modular layer design.\n",
        "    We assume an input dimension of D, a hidden dimension of H, and perform\n",
        "    classification over C classes.\n",
        "\n",
        "    The architecture should be input - fc - relu - fc - softmax\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.hidden_dim = hidden_dim\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        # Hint: np.random.normal\n",
        "        ############################################################################\n",
        "        W1 = np.random.normal(0, weight_scale, size=(input_dim, hidden_dim))\n",
        "        b1 = np.zeros(hidden_dim)\n",
        "        W2 = np.random.normal(0, weight_scale, size=(hidden_dim, num_classes))\n",
        "        b2 = np.zeros(num_classes)\n",
        "        self.params['W1'] = W1\n",
        "        self.params['b1'] = b1\n",
        "        self.params['W2'] = W2\n",
        "        self.params['b2'] = b2\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Do a forward and backward pass through the network.\n",
        "        i.e. Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        W1 = self.params['W1']\n",
        "        b1 = self.params['b1']\n",
        "        W2 = self.params['W2']\n",
        "        b2 = self.params['b2']\n",
        "        input_2_hidden1, cache1 = fc_forward(X, W1, b1)\n",
        "        hidden1_out, cache2 = relu_forward(input_2_hidden1)\n",
        "        scores,cache3 = fc_forward(hidden1_out, W2, b2)\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        grads = {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss, dscores = softmax_loss(scores, y)\n",
        "        dh2, dw2, db2 = fc_backward(dscores, cache3)\n",
        "        dx = relu_backward(dh2, cache2)\n",
        "        _, dw1, db1 = fc_backward(dx, cache1)\n",
        "        grads['W2'] = dw2\n",
        "        grads['b2'] = db2\n",
        "        grads['W1'] = dw1\n",
        "        grads['b1'] = db1\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXRrhPGYz5rI"
      },
      "source": [
        "# Training and evaluation functions\n",
        "In the following section, we provide you helper functions to train your neural network model and evaluate your trained model on test/validation set. \n",
        "\n",
        "You are not required to implement anything here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxse8axDzvIz"
      },
      "source": [
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_s2l1muddC8"
      },
      "source": [
        "# Load CIFAR-10 dataset\n",
        "\n",
        "Run the following cell to load CIFAR-10 dataset, preprocess the data and prepare train/test split. \n",
        "\n",
        "You're not required to implement anything here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSdzycBkdddp"
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUJvfy5X2AxS"
      },
      "source": [
        "# Train your MLP Classifier model\n",
        "\n",
        "Run the cell below to initialize and train your MLP Classifier model.\n",
        "\n",
        "You're not required to implement anything here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw9z58ACdpb0",
        "outputId": "37b9fcbd-adec-4c9e-88d4-981772873d92"
      },
      "source": [
        "np.random.seed(100)\n",
        "# initialize model\n",
        "model = MLPClassifier(hidden_dim =250, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 0.1,\n",
        "    lr_decay=.92, num_epochs=10, \n",
        "    batch_size=150, print_every=1000)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 2660) loss: -2.293548\n",
            "(Epoch 0 / 10) train acc: 0.131000; val_acc: 0.148700\n",
            "(Epoch 1 / 10) train acc: 0.425000; val_acc: 0.394700\n",
            "(Epoch 2 / 10) train acc: 0.499000; val_acc: 0.455600\n",
            "(Epoch 3 / 10) train acc: 0.508000; val_acc: 0.452900\n",
            "(Iteration 1001 / 2660) loss: 0.114148\n",
            "(Epoch 4 / 10) train acc: 0.640000; val_acc: 0.481600\n",
            "(Epoch 5 / 10) train acc: 0.625000; val_acc: 0.479100\n",
            "(Epoch 6 / 10) train acc: 0.617000; val_acc: 0.495700\n",
            "(Epoch 7 / 10) train acc: 0.625000; val_acc: 0.496600\n",
            "(Iteration 2001 / 2660) loss: -0.007153\n",
            "(Epoch 8 / 10) train acc: 0.630000; val_acc: 0.480200\n",
            "(Epoch 9 / 10) train acc: 0.683000; val_acc: 0.503800\n",
            "(Epoch 10 / 10) train acc: 0.732000; val_acc: 0.503400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IPSfJ9SeiP9"
      },
      "source": [
        "# Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n",
        "\n",
        "*Hint - If you implemented correctly, you should obtain atleast 45% accuracy on test set.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sqErweYekZj",
        "outputId": "ab0c0e8d-244a-4926-9836-fd3cd16d65e2"
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.5038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61QIRkyVepr2"
      },
      "source": [
        "# Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "TpJTasbZevsO",
        "outputId": "9999191b-c90d-4ac3-e732-9f935d94a408"
      },
      "source": [
        "plt.plot(train_acc_history, label = \"Train accuracy\")\n",
        "plt.plot(val_acc_history, label = \"Validation accuracy\")\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"% accuracy\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV1bXA8d/KTEiYQsIUIMzIFBICKoqACuIERUVBax3qgIpTa63aPq36+tqqtQ51olZtrRQcERUERRAVlTCEIWEKgxCGJAQICSHjXe+PcxMuIQkBcnOT3PX9fPLJPfO6Efc6Z++z9xZVxRhjjP8K8HUAxhhjfMsSgTHG+DlLBMYY4+csERhjjJ+zRGCMMX4uyNcBnKy2bdtqXFycr8MwxphGZcWKFftUNbqqbY0uEcTFxbF8+XJfh2GMMY2KiPxU3TarGjLGGD9nicAYY/ycJQJjjPFzja6NoColJSVkZGRQWFjo61BMAxEWFkZsbCzBwcG+DsWYBq9JJIKMjAwiIyOJi4tDRHwdjvExVSUnJ4eMjAy6devm63CMafCaRNVQYWEhUVFRlgQMACJCVFSUPSEaU0tNIhEAlgTMMezfgzG112QSgTHGNFW5BSU89fkGfso57JXzN4k2Al/LycnhggsuAGDv3r0EBgYSHe104Fu2bBkhISHVHrt8+XL+/e9/88ILL9RLrMaYxuNIcRlvLd3OK4vTySsqpUOrZlwf1bzOr2OJoA5ERUWRkpICwB/+8AciIiJ44IEHKraXlpYSFFT1nzopKYmkpKR6ifNk1RS3McZ7SspczEreyQsLN5OVV8T5fWN4YGwf+nVs4ZXrWdWQl9x4441MnTqVM888kwcffJBly5Zx9tlnk5CQwPDhw9m4cSMAixcv5rLLLgOcJHLzzTczatQounfvXu1Twh133EFSUhL9+/fnscceq1ifnJzM8OHDiY+PZ9iwYeTl5VFWVsYDDzzAgAEDGDRoEC+++CLgDNWxb98+wHkqGTVqVEUM119/Peeccw7XX38927dvZ8SIESQmJpKYmMjSpUsrrveXv/yFgQMHEh8fz0MPPcSWLVtITEys2L558+Zjlo0xNXO5lI9TdnHhs1/z+9nr6BoVzntTz+aNG4d6LQlAE3wiePyTVNJ2H6rTc/br2ILHLu9/0sdlZGSwdOlSAgMDOXToEN988w1BQUF8+eWXPPLII3zwwQfHHbNhwwYWLVpEXl4effr04Y477jjuXfg//vGPtGnThrKyMi644ALWrFlD3759ueaaa5g1axZDhw7l0KFDNGvWjOnTp7N9+3ZSUlIICgpi//79J4w7LS2Nb7/9lmbNmlFQUMAXX3xBWFgYmzdvZsqUKSxfvpx58+bx8ccf8+OPPxIeHs7+/ftp06YNLVu2JCUlhcGDB/Pmm29y0003nfTfzRh/o6os3pjNU/M3sn7PIfq2j+TNG4cyqk90vbz40OQSQUMyadIkAgMDAcjNzeWGG25g8+bNiAglJSVVHnPppZcSGhpKaGgoMTExZGZmEhsbe8w+7777LtOnT6e0tJQ9e/aQlpaGiNChQweGDh0KQIsWzt3Dl19+ydSpUyuqeNq0aXPCuMePH0+zZs0Ap7PetGnTSElJITAwkE2bNlWc96abbiI8PPyY895yyy28+eabPPvss8yaNYtly5ad1N/MGH+zfPt+nvp8I8u276dLm3CenzyYywd1JCCg/t58a3KJ4FTu3L2lefOjjTr/8z//w+jRo/noo4/Yvn17RVVMZaGhoRWfAwMDKS0tPWb7tm3beOaZZ0hOTqZ169bceOONp/S+fFBQEC6XC+C44z3j/tvf/ka7du1YvXo1LpeLsLCwGs975ZVX8vjjj3P++eczZMgQoqKiTjo2Y/zB+j2HeGb+RhZuyCI6MpQnfzaAa5I6ExJU/zX21kZQT3Jzc+nUqRMAb7311imf59ChQzRv3pyWLVuSmZnJvHnzAOjTpw979uwhOTkZgLy8PEpLSxkzZgyvvfZaRUIprxqKi4tjxYoVAFVWUXnG3aFDBwICAnj77bcpKysDYMyYMbz55psUFBQcc96wsDAuuugi7rjjDqsWMqYKO3IKuG/mKi554RuSt+/nwXF9+Po3o7j+rK4+SQJgiaDePPjggzz88MMkJCQcd5d/MuLj40lISKBv375ce+21nHPOOQCEhIQwa9Ys7r77buLj4xkzZgyFhYXccsstdOnShUGDBhEfH8+MGTMAeOyxx7j33ntJSkqqqL6qyp133sm//vUv4uPj2bBhQ8XTwrhx4xg/fjxJSUkMHjyYZ555puKY6667joCAAMaOHXvK39OYpibrUCH/M3sd5/91MZ+n7mXqyB588+D53DmqJ+Ehvq2cEVX13slFxgHPA4HA66r650rb/waMdi+GAzGq2qqmcyYlJWnliWnWr1/PGWecUWdxm9PzzDPPkJuby5NPPunTOOzfhWkIco+U8NrXW3jzu+2UlLmYPKwz95zfi5gWNVez1jURWaGqVb6r7rU0JCKBwEvAGCADSBaROaqaVr6Pqt7vsf/dQIK34jH1Y+LEiWzZsoWvvvrK16EY41OencEOFZYyYXBHfjWmN1290CHsdHnzeWQYkK6qWwFEZCYwAUirZv8pwGPVbDONxEcffeTrEIzxqfruDFYXvJkIOgE7PZYzgDOr2lFEugLdgCpvI0XkNuA2gC5dutRtlMYYUwdcLuWTNbt59otN/JRTQFLX1rx0XSJD4078yravNZTXRycD76tqWVUbVXU6MB2cNoL6DMwYY2pSVWewN25MYnSfmEYzCq43E8EuoLPHcqx7XVUmA3d5MRZjjKlzDaEzWF3wZiJIBnqJSDecBDAZuLbyTiLSF2gNfO/FWIwxps40pM5gdcFrUatqKTANmA+sB95V1VQReUJExnvsOhmYqd58j9XLRo8ezfz5849Z99xzz3HHHXdUe8yoUaMofw32kksu4eDBg8ft84c//OGY9/OrMnv2bNLSjra/P/roo3z55ZcnE74xppZ27m94ncHqglfbCFR1LjC30rpHKy3/wZsx1IcpU6Ywc+ZMLrrooop1M2fO5KmnnqrV8XPnzj3xTtWYPXs2l112Gf369QPgiSeeOOVz+UpZWVmNndqM8bX8olJeXpTO699uI0Bg6sgeTD2vBy3Dg098cCPQeFNYA3LVVVfx2WefUVxcDMD27dvZvXs3I0aMqHbIaE+eQ0L/8Y9/pHfv3px77rkVQ1UD/OMf/2Do0KHEx8dz5ZVXUlBQwNKlS5kzZw6/+c1vGDx4MFu2bOHGG2/k/fffB2DhwoUkJCQwcOBAbr75ZoqKiiqu99hjj5GYmMjAgQPZsGHDcTGdzPDTAOnp6Vx44YXEx8eTmJjIli1bjhliG2DatGkVw2vExcXx29/+lsTERN57770qvx9AZmYmEydOJD4+nvj4eJYuXcqjjz7Kc889V3He3/3udzz//PMn9x/NmFpwuZT3lu9k9DOLeXnxFi4b2IFFD4zit+P6NpkkAA3nraG6M+8h2Lu2bs/ZfiBc/OdqN7dp04Zhw4Yxb948JkyYwMyZM7n66qsRkSqHjB40aFCV51mxYgUzZ84kJSWF0tJSEhMTGTJkCABXXHEFt956KwC///3v+ec//8ndd9/N+PHjueyyy7jqqquOOVdhYSE33ngjCxcupHfv3vziF7/glVde4b777gOgbdu2rFy5kpdffplnnnmG119//ZjjY2Jiaj38NDjDSjz00ENMnDiRwsJCXC4XO3fupCZRUVGsXLkScGZ5q+r73XPPPYwcOZKPPvqIsrIy8vPz6dixI1dccQX33XcfLpeLmTNn2iinps4lb9/PE5+ksXZXLgldWjH9+iEkdGnt67C8wp4I6kh59RA41UJTpkwBnCGjExMTSUhIIDU19Zj6/Mq++eYbJk6cSHh4OC1atGD8+KNNKevWrWPEiBEMHDiQd955h9TU1Brj2bhxI926daN3794A3HDDDSxZsqRi+xVXXAHAkCFD2L59+3HHl5SUcOuttzJw4EAmTZpUEXdVw0/n5eWxa9cuJk6cCDgDz5Vvr8k111xzwu/31VdfVbS1BAYG0rJlS+Li4oiKimLVqlUsWLCAhIQEG+XU1JmMAwXcNWMlk179nn35RTw/eTAf3jG8ySYBaIpPBDXcuXvThAkTuP/++1m5ciUFBQUMGTKkzoaMBmfGs9mzZxMfH89bb73F4sWLTyve8uGuqxrqGk5++OmqeA51DTUPd32y3++WW27hrbfeYu/evdx8880nHZsxlR0uKuXVr7cwfclWRODeC3px+8juPh8Qrj7YE0EdiYiIYPTo0dx8880VTwPVDRldnfPOO4/Zs2dz5MgR8vLy+OSTTyq25eXl0aFDB0pKSnjnnXcq1kdGRpKXl3fcufr06cP27dtJT08H4O2332bkyJG1/j4nM/x0ZGQksbGxzJ49G4CioiIKCgro2rUraWlpFBUVcfDgQRYuXFjt9ar7fhdccAGvvPIK4DQq5+bmAs6YRp9//jnJycnHNNIbc7JcLuWDFRmc/9fFvPhVOuMGtOerX4/i/jG9/SIJgCWCOjVlyhRWr15dkQiqGzK6OomJiVxzzTXEx8dz8cUXV8w2BvDkk09y5plncs4559C3b9+K9ZMnT+bpp58mISGBLVu2VKwPCwvjzTffZNKkSQwcOJCAgACmTp1a6+9yssNPv/3227zwwgsMGjSI4cOHs3fvXjp37szVV1/NgAEDuPrqq0lIqH5Mweq+3/PPP8+iRYsYOHAgQ4YMqaiiCgkJYfTo0Vx99dX2xpE5ZSt+2s/El7/j1++tpn3LZnxwx3Cen5xAx1bNfB1avfLqMNTeYMNQGwCXy1XxxlGvXr2q3Mf+XZjq7Dp4hL/M28Cc1btp1yKU347ry88Gd2p0PYJPhk+GoTbGW9LS0rjsssuYOHFitUnAmKoUFJfy6tdbmb5kC6pwz/k9mTqqh99UAVXHv7+9aZT69evH1q1bfR2GaURcLuXj1bv4y7yN7D1UyOXxHXno4r508rMqoOo0mUSgqo1mpD/jfY2tytN4z6odB3j8kzRSdh5kUGxL/n5tAkmNYGjo+tQkEkFYWBg5OTlERUVZMjCoKjk5Oaf0yqtpOvbkHuGpzzfy0apdxESG8sykeK5IaNrtAKeqSSSC2NhYMjIyyM7O9nUopoEICwsjNjbW12EYHzhSXMb0JVt59estlKkybXRP7hjVg+ahTaK484om8ZcJDg6mW7duvg7DNAAHDhezJ7eQDtHNCQ6210r9iaoyZ/Vu/jJvA7tzC7l0UAceGteXzm1O3Mvd3zWJRGAMOHeCV726lC3ZhxGBzq3D6RUTQc9KP5FhTWewMONYvfMgj3+SysodBxnQqQXPTU5gWDdrB6gtSwSmyfjTvPVsyT7Mwxf3paC4jPSsfNKz8lmyOZuSsqONx+1bhNGrXQQ9oiPo1S6CntER9GoXSZvmIT6M3pyKvbmFPDV/Ax+u3EV0ZChPXTWIqxJjrR3gJFkiME3Coo1Z/Pv7n/jlud24fWSPY7aVlrnYsb+A9Kx8Nmfls8X9+93lOykoPjpNdpvmIRVPDb0qfkfSrkWovYTQwBSWlPGPJVt5ebHTDnDnqB7cObonEdYOcErsr2Yavf2Hi3nw/TX0aRfJby7qc9z2oMAAukdH0D06grH9j653uZTduUcqnhzKE8Vna/aQe6SkYr/I0CB6eCaHdhH0jI4ktnUzu/M8BUWlZeQXlpJf5P7x+JxXWMphj8/l2w8XH7t88EgxhSUuLh7QnkcuOcPaAU6TJQLTqKkqD32whtyCEv510zDCTqKBOCBAiG0dTmzrcEb1iTnmnNn5RcckiPSsfBZvyua9FRkV+4UFB9C97dHqpR4xEcREhhIVEUpURAiRoUFN7knC5VJyj5SQc7iIffnFHDhcTF6lwvyYwr2w1NleVMLhIicBFJe5TnidAIGI0CAiw4KJCA0iIiyIFs2C6dSqWcXy2H7tOLO7DT9eFywRmEbtveUZLEjL5JFL+tKvY4s6OaeIEBMZRkxkGMN7tD1mW25BCenZec7TQ2Y+6dn5LN9+gI9Tdh93npCgANo2D6lIDFHNQ2kbEUJURAhtI9wJo7nzuU3zEJ/NeVtYUsa+/CJy8osrCviK5fwicg4Xs8/9ef/hYkpd1XfWCwsOICI0mMiwIKfADg2iU6tmRIZFVhTgTgEfRPMQZznSY33572bBgU0uiTZklghMo7Ujp4DHP0nl7O5R3HJu93q5ZsvwYIZ0bcOQrse+kVJQXMr2fQVOAXq4iH15xew7fGxhujkzn+z8IopLq74jbhEWRNuIUHeSCKmUPI6ub9s8lBbNqn/aKHMpBwuKnQI8r4h9h90xeBT0OflHfx/2aCfxFB4SWJG0OrVqRnxsy4qYoiJCiI4IpXXzECLDgogMDaZ5aCBBgTagcWNkicA0SqVlLu5/N4WAAOGvV8f7vK4+PCSoVk8kqkp+UWmlQrn8DvxooZ2elc+P24o5UFBMVaNlBAcKbZofLZTLXFpxzv2Hi6nqpj0woPwYp3BP6BJ+TKEe5U44Uc2dJOTvA7H5E/svbRqlVxZvYcVPB3h+8uBGNXa8iBAZFkxkWDBxbZufcP/SMhf7C4rdTxaV7+idu/x9h4sJChC6RoWT2LU10RFVVUeF0qpZsM8TpmmYLBGYRmf1zoM8v3Az4+M7MmFwJ1+H41VBgQEV7RXGeItXK/REZJyIbBSRdBF5qJp9rhaRNBFJFZEZ3ozHNH4FxaXcPyuFmMhQnpwwwNfhGNMkeO2JQEQCgZeAMUAGkCwic1Q1zWOfXsDDwDmqekBEYqo+mzGO/5u7nm05h3nnljNpGW5DRRhTF7z5RDAMSFfVrapaDMwEJlTa51bgJVU9AKCqWV6MxzRyX23I5D8/7ODWEd2Pe63TGHPqvJkIOgE7PZYz3Os89QZ6i8h3IvKDiIzzYjymEduXX8SD76+hb/tIfj22t6/DMaZJ8XVjcRDQCxgFxAJLRGSgqh703ElEbgNuA+jSpUt9x2h8zOk9vJZDhaX855YzCQ2y4aWNqUvefCLYBXT2WI51r/OUAcxR1RJV3QZswkkMx1DV6aqapKpJ0dHRXgvYNEyzknfy5fpMHryoD33b103vYWPMUd5MBMlALxHpJiIhwGRgTqV9ZuM8DSAibXGqimxWclNh+77DPPFpGuf0jOLmc2zyIWO8wWuJQFVLgWnAfGA98K6qporIEyIy3r3bfCBHRNKARcBvVDXHWzGZxqW0zMV9s1IIChCemeT73sPGNFVebSNQ1bnA3ErrHvX4rMCv3D/GHOPvi9JJ2XmQv1+bQIeWjaf3sDGNjY0QZRqkVTsO8OJX6UxM6MRlgzr6OhxjmjRLBKbBOVzk9B5u3yKMxyf0P/EBxpjT4uvXR405zv9+tp6f9hcw89azaGETzRvjdfZEYBqUL9My+e+yHdx+Xg+bfcqYemKJwDQY2XlF/PaDNfTr0IJfjbHew8bUF6saMg1C+dzD+UWlzJw82GfTNhrjj+z/NtMgzFi2g4Ubsnjo4r70ahfp63CM8SuWCIzPbc3O538/Xc+IXm254ew4X4djjN+xRGB8qqTMxf2zUggNDrDew8b4iLURGJ968at0Vmfk8vJ1ibRrYdMxGuML9kRgfGbFTwf4+1ebuTIxlksGdvB1OMb4LUsExifyi0r51bspdGzVjD+M7+frcIzxa1Y1ZHziyU/S2Lm/gFm3n02k9R42xqfsicDUu/mpe5m1fCd3jOrB0Lg2vg7HGL9nicDUq6y8Qh7+cC0DOrXg3gus97AxDYElAlNvVJUH319DQXEpz12TYL2HjWkg7P9EU2/+88NPLN6YzSOXnEHPmAhfh2OMcbNEYOpFelY+f5y7npG9o7n+rK6+DscY48ESgfG64lKn93Cz4ECevmoQItZ72JiGxF4fNV73wsLNrN2Vy6s/H0KM9R42psGxJwLjVcu37+flxelcnRTLuAHtfR2OMaYKlgiM1+QVlnD/uynEtg7n0ctt7mFjGiqrGjJe88Qnaew6cIT3pg4nItT+qRnTUJ3wiUBEBtZHIKZp+XzdHt5bkcG00T0Z0rW1r8MxxtSgNrdpL4tIKPAW8I6q5tb25CIyDngeCAReV9U/V9p+I/A0sMu96u+q+nptz298J7+olOy8IrIOFZKVV+T+KST7kPM5ZedB4mNbcvcFvXwdqjHmBE6YCFR1hIj0Am4GVojIMuBNVf2ipuNEJBB4CRgDZADJIjJHVdMq7TpLVaedWvimLqkqBwpKyMorJOtQkVPQuwv4rLwidyHvfC4oLjvu+JDAAKIjQ4lpEcqoPtE8eFFfggOtGco0EYWHYMf3sG+TsywBgIBIFb85drnGfaWKfal6W/tB0KZbnX+1WlXcqupmEfk9sBx4AUgQ52XwR1T1w2oOGwakq+pWABGZCUwAKicC42WlZS725RdXFPBZeeWF/NG7+exDhWTnF1FSpscdHxEaRExkKNGRoQyMbUV0hFPYx0SGEhMZVvG5ZbNg6yNgmo7iw7DjB9j+DWxbArtTQI+/AapXlz4LbX5Z56c9YSIQkUHATcClwBfA5aq6UkQ6At8D1SWCTsBOj+UM4Mwq9rtSRM4DNgH3q+rOKvYxJ6m0zMXTCzbywYpd5BwuQo8v32kdHlxRkPeIjnI+R5YX8kc/h4dYQ69fUnX/lIG6wOX+XbHsgsBgCGvh60jrRkkhZCyDbd84hX/GcnCVQEAQdEqCEb+CuBHQYRBIIFD+93E5x6seXVfdb3XVsA8n3rdFR6989dr8H/4i8DrO3f+R8pWqutv9lHA6PgH+q6pFInI78C/g/Mo7ichtwG0AXbp0Oc1LNn37DxczbcZKlm7JYVz/9vRuH+m+ew8lpoVTwLeNCLVB3xqr0iLI3giZqZC5DrLS4MgBd0F9goL7mOXy7a6q9y8v4E4ksiO06wcx/aDdAOdz294QFOrdv8PpKi2GXSuO3vHvXAZlRU7VTIfBcPZd0G0EdD4LQpv22Fi1SQSXAkdUnWciEQkAwlS1QFXfruG4XUBnj+VYjjYKA6CqOR6LrwNPVXUiVZ0OTAdISkqq4t7WlFu3K5fb315Bdn4RT181iElJnU98kGmYVOHQLneB7/Gzb9PRKorAUIjuA5HtnQJMAp265IBAj+UAj+WASsue26WK/cuXA6o+X0kBZG2ArFSnMC0rduIKCIKoXk5SaNcfYvo7n1t2PlqHXt/KSmHPatj2tVP47/jBiR+B9gNg2K3OHX/XsyGspW9i9JHaJIIvgQuBfPdyOLAAGH6C45KBXiLSDScBTAau9dxBRDqo6h734nhgfS3jNlX4OGUXv/1gDa3DQ3jv9rOJ79zK1yGZ2io+DFnrnTv8ikJ/HRR6vKTXsotTqPa91PndbgC06Q6BDaTqrqwEcrYcfUrJTIOdybDug6P7hLaEmDPc8fc7miC8UfC6XJC51klQ276Bn5ZCcZ6zLfoMSLjeuePveg6E+/cESbX5FxSmquVJAFXNF5HwEx2kqqUiMg2Yj/P66BuqmioiTwDLVXUOcI+IjAdKgf3AjafyJfxdaZmLP83bwD+/3cawbm146dpEoiMb+GO5v3K54MA2d0GZerTg378NcD/shkQ41Sz9rzha4MecAc0aeGIPDIaYvs6Pp8JDR5Nc+fde+z4s90xynd1VS/2P/kT1dM5ZW6rOdcqrerZ/C4UHnW1RPWHgVdDtPOeuPyL69L9vEyJaVSui5w4i3wF3q+pK9/IQnPf9z66H+I6TlJSky5cv98WlG6Sc/CLu/u8qlm7J4cbhcfzu0jP8+5XNonzIz3TqpwNDISjE+R0Y4lRv1KcjB5y7Ys8CP2s9lBx27yAQ1cOjsHcXhK261n+s9a1ytVd5gti3CVylzj6BIU5bQ7v+x7Y/RHZwqpdUISfdfcfvLvgL9jnHturq3O13Gwlx53qtkbUxEZEVqppU1bbaPBHcB7wnIrsBAdoD19RhfOYUebYHPDMpnquGxPo6JN/Iy4SNc52frYuP1lNXFhDsThDBxyaJIHeiqPK3e//K6yqO9TiHujwacVPhUMbRazdr7RRkidcfveON7gshzevlT9TgiEDLWOen90VH15cWO8mg4okp1Sng18w6uk9YK+dvd/AnyHPXLLfoBD0vdAr/uBHQ2ua8OBm16VCWLCJ9gT7uVRtVtcS7YZkTmb3KaQ9o0zyE96eezaDYBl5tUNeyN8GGT53CPyPZWdc6DobeCu0HOsmgrNh5w6asyClgPH+XFR+/rrQISgudevmKY0uObitfV9O75AFBzl1s17OP3um363/0LtbULCjEabhtP+DY9eVPV1lpztNV9kboOtwp9Lud57SV2N/3lNW2lakP0A8IAxJFBFX9t/fCMtWp3B7w8nWJtI3wg/YAV5nzXnd54Z+T7qzvmACjf+80oMacUT+FgausUoJx/6gLWnVp+K9NNkbNWkPcOc6PqXO16VD2GDAKJxHMBS4GvgUsEdSznPwips1Yxfdb/aQ9oOQIbP0aNn4GG+fB4WznjrvbeXDmVOhzCbTsVP9xBQRCSDjOC3TGNH61eSK4CogHVqnqTSLSDviPd8MylflNe0DBftg03yn80xc673mHtnDqf/teCr3G+N073sZ4W20SwRFVdYlIqYi0ALI4tqOY8bLy9oCo5iF8MHU4A2ObWEF4YDtscDf2/rTUqYOP7AjxU5zCP26EU3dsjPGK2iSC5SLSCvgHsAKnY9n3Xo3KAMe2B5zZrQ0vNZX2AFWnh+eGz5zCP3Odsz6mH5x7P/S9BDokNP1XKI1pIGpMBO4RRv+kqgeBV0Xkc6CFqq6pl+j8WJNrDygrcV4D3DjXufs/lOEMUdDlbBj7R6fwb9Pd11Ea45dqTASqqiIyFxjoXt5eH0H5O8/2gL9OiufKU20P2Pg5bF5w9P338p/Aaj5XLIc5VTFBYe5jPZdDa3+nXngI0r90Cv/NC5zXMoOaQY/zYfQjzvvjzdue2nczxtSZ2lQNrRSRoaqa7PVoDB+tyuChD9aeXnvA4RyY9xtnjJeQSOeVytLC6jtanazyjlnVJhJ356qMZOea4VHQ93Knvr/7KH/4ykUAABb6SURBVPcbN8aYhqI2ieBM4DoR+Qk4jNO7WFV1kFcj8zOlZS7+b+4G3vjuNNoDVCH1I5j7G+fue/TvnDr38vFaXC73O+/lnaeKPN6HL6zUsaqKbeXJ5JjlqvYvct61H3abU/h3PtN55dIY0yDVJhFcdOJdzOnIyS/irhkr+WHrfm46J45HLjmF9oC8TJj7a1j/idPJasInzrgsngICICAMgsPqLnhjTKNXm0Rg4/97UXl7wL78Ip69Op4rEk+yPUAV1rwLn/8Wigvgwsfh7GkNZ2hiY0yDV5vS4jOcZCA4Q0x0AzYC/b0Yl1/wbA94/1TaAw7thk/vh02fQ+wwmPASRPf2TrDGmCarNoPODfRcFpFE4E6vReQHSspc/MndHnBWd2f+gKiTaQ9QhVVvw/zfOa9lXvQnOPN2q4c3xpySk64/cE9cX9Uk9KYWPNsDbj6nGw9f0vfk2gMO7oA598DWRU6P2/Ev2Pv3xpjTUptB537lsRgAJAK7vRZRE7Y2I5fb315OzuHik28PcLlgxRvwxWPO8qXPwpCbrPetMea01eaJINLjcylOm8EH1exrqvHhygwe/nAtbSNC+eCO4QzodBLtAfu3Ok8B279xOmNd/rwz3LExxtSB2rQRPF4fgTRlf5q7nteWbD359gBXGfz4Gix8wunhO/7vkPBzm4DDGFOnalM19AUwyT3eECLSGpipqta/oBa2Zufz2pKtXJ0Uy/9NHEhQbdsDsjfBnGmw80foPQ4u+5vNu2qM8YraVA1FlycBAFU9ICIxXoypSVmQlgnAfRf2rl0SKCuF71+ERX9yhmK44h8wcJI9BRhjvKY2iaBMRLqo6g4AEemKdTKrtfmpexnYqSUdWzU78c6ZafDxnbB7FZxxOVzyV4hs5/0gjTF+rTaJ4HfAtyLyNU6nshHAbV6NqonIOlTIqh0HeWDsCTp5lZXAt3+Dr59yZt+a9Bb0n1gvMRpjTG0aiz93dyI7y73qPlXd592wmobyaqGL+revfqc9q2H2XZC5FgZcBRc/Bc2j6ilCY4xx+gXUSEQmAiWq+qmqfgqUisjPanNyERknIhtFJF1EHqphvytFREUkqfahN3zzU/fSrW1zesZEHL+xtAgWPgnTRzuTsk+eAVf905KAMabe1eYVlsdUNbd8wd1w/NiJDhKRQOAl4GKgHzBFRPpVsV8kcC/wY22Dbgxyj5Tw/ZYcxvZvh1Ru6M1YAa+dB988A/GT4a4fnOGajTHGB2qTCKrapzZtC8OAdFXdqqrFwExgQhX7PQn8BSisxTkbjcUbsyh1KWP7eVQLlRyBBf8D/7wQivLgug/gZy9Ds9a+C9QY4/dqkwiWi8izItLD/fMsziT2J9IJ2OmxnOFeV8Hd9tBZVT+r6UQicpuILBeR5dnZ2bW4tO8tSM0kOjKUhM6tnBU7foBXz4WlL0DiDXDnD9DrQt8GaYwx1C4R3A0UA7PcP0XAXad7YREJAJ4Ffn2ifVV1uqomqWpSdHT06V7a6wpLyli8MYsx/doRIMAXj8Ib45zZvX7xMVz+HIS18HWYxhgD1O6tocNAtQ29NdgFdPZYjnWvKxcJDAAWu+vQ2wNzRGS8qi4/hes1GN+l7+NwcZnzttCe1fDd8xB/LVzyNIRW0XBsjDE+VJshJqKBB3EmoqmY41BVzz/BoclALxHphpMAJgPXehyfC7T1uM5i4IHGngTAqRaKDA3i7O5RsOBpZ0L3cf9nScAY0yDVpmroHWADzsxkjwPbcQr5GqlqKTANmA+sB95V1VQReUJExp9yxA1cmUv5cn0mo/vGEEIJrH3XeSPIGoSNMQ1Ubd7+iVLVf4rIvar6NfC1iJwwEQCo6lxgbqV1j1az76janLOhW/HTAXIOFzvVQps+hyMHYPB1vg7LGGOqVZsnghL37z0icqmIJABtvBhTozY/dS8hQQGM7BMNKTMgsgP0GO3rsIwxplq1eSL4XxFpifN2z4tAC+B+r0bVSKkqC9L2cm7PtkQU58DmL+Cce2wuYWNMg1abt4Y+dX/MBezWtgbr9+Sxc/8R7hrV02kb0DLnbSFjjGnAbMLbOjQ/dS8BAheeEQOr3oHYoRB9gpFHjTHGxywR1KEFaZkkdW1D20NpkL3eGomNMY2CJYI6snN/Aev3HGJs/3ZOI3FQmM0pYIxpFGqdCETkLBH5XEQW13YYan8yP3UvAGN7t4K170Hfy6BZKx9HZYwxJ1ZtY7GItFfVvR6rfgVMxJml7Edgtpdja1QWpGbSt30kXfZ9DYUHIcGqhYwxjUNNTwSvisijIlI+rMRB4CqcZHDI65E1Ivvyi0j+ab/TiSxlBrToBN1G+josY4yplWoTgar+DFgFfCoivwDuA0KBKMCqhjwsXJ+JKlwSp5D+pTPZjPUdMMY0EjW2EajqJ8BFQEvgI2CTqr6gqo1jUoB6Mj81k9jWzeidOQ/UZW8LGWMalWoTgYiMF5FFwOfAOuAaYIKIzBSRHvUVYEOXX1TKt+n7uKhfOyRlBnQ+C6Lsz2OMaTxq6ln8vzjTTTYD5qvqMODXItIL+CPOsNJ+7+uN2RSXupjYLhNWbITLX/B1SMYYc1JqSgS5wBVAOJBVvlJVN2NJoMKCtL20aR5Cv8xPIKiZ9R0wxjQ6NbURTMRpGA7CY0IZc1RxqYuvNmQxrk9LAtZ9AP3G2xSUxphGp9onAlXdhzPaqKnG91tzyCssZUrLdVCUC4MtXxpjGh8bYuI0LEjdS3hIoFMt1LIzxJ3n65CMMeakWSI4RS6X8kVaJhN6QOC2xRA/BQLsz2mMaXxqMzGNqUJKxkGy8oq4vlmyu+/AFF+HZIwxp8RuYU/R/NS9BAVAn72fQJfh0Ka7r0MyxphTYongFKgqC1IzuT42i8D96dZIbIxp1CwRnIL0rHy27TvMlJBvITgc+tvQS8aYxssSwSmYn7qXUIrpmTUf+k2A0Ehfh2SMMafMEsEpWJCWye3RqQQU51m1kDGm0fNqIhCRcSKyUUTSReShKrZPFZG1IpIiIt+KSD9vxlMXdh88wpqMXCYFfQOtukDXc30dkjHGnBavJQIRCQReAi4G+gFTqijoZ6jqQFUdDDwFPOuteOrKF2mZdCCH2AM/Qvy11nfAGNPoebMUGwakq+pWVS0GZgITPHdQVc+ZzpoD6sV46sT81L3c2vJHBLW+A8aYJsGbiaATsNNjOcO97hgicpeIbMF5IrinqhOJyG0islxElmdn+25OnAOHi/lxWw4TZTHEjYDWcT6LxRhj6orP6zVU9SVV7QH8Fvh9NftMV9UkVU2Kjo6u3wA9fLUhi8G6kdaFGdZIbIxpMryZCHYBnT2WY93rqjOTBj4X8vzUvfyi2XdocHM4Y7yvwzHGmDrhzUSQDPQSkW4iEoIzmc0czx3cs52VuxTY7MV4TsuR4jKSN+/kIpYi/X8GoRG+DskYY+qE1wadU9VSEZkGzAcCgTdUNVVEngCWq+ocYJqIXAiUAAeAG7wVz+lasjmbkWXLCAsssMnpjTFNildHH1XVucDcSuse9fh8rzevX5fmp+5lcsg3aOs4pMvZvg7HGGPqjM8bixuD0jIXaWmpDGMdYn0HjDFNjJVotbBs237GlHxFgPUdMMY0QZYIamFB6l4mBS2hrOsIZ1gJY4xpQiwRnICqkrXuK7pIFoGJP/d1OMYYU+csEZzAul2HGHXkS0qCmsMZl/s6HGOMqXOWCE7gqzVbuTTwB1xn/AxCmvs6HGOMqXM2ef0JFK6ZTXMpgqTrfR2KMcZ4hT0R1GBrdj7nHV7AoWadoctZvg7HGGO8whJBDb5fsZKzA9PQhOtAxNfhGGOMV1giqEHg2pm4EFoOs7eFjDFNlyWCamTlFnBO3gIyWg2FVp1PfIAxxjRSlgiqsfrbuXQOyCZoiDUSG2OaNksE1QhLm0k+4XQ480pfh2KMMV5liaAKhw4dIDF/CZujxyDWd8AY08RZIqjClkX/obkU0WyoVQsZY5o+SwRViNjwLj/Rkd5DLvB1KMYY43WWCCopykqn15E1rG9/OQGB9ucxxjR9VtJVsvvrNyhToeWZNh2lMcY/WCLw5HLRatP7fM8gEgcO8HU0xhhTLywReCjbtoTWJZls7jie0KBAX4djjDH1wkYf9XDguzcJ0XBihl7h61CMMabe2BNBucJDtNw+j89cwzmvnw0pYYzxH5YI3DT1I4JdRaR3Gk9kWLCvwzHGmHpjVUNuR5LfZrerIz0TRvk6FGOMqVdefSIQkXEislFE0kXkoSq2/0pE0kRkjYgsFJGu3oynWjlbCN+bzPuu87iwX3ufhGCMMb7itUQgIoHAS8DFQD9gioj0q7TbKiBJVQcB7wNPeSueGqXMoIwAtrS/jOjIUJ+EYIwxvuLNJ4JhQLqqblXVYmAmMMFzB1VdpKoF7sUfgFgvxlM1Vxmlq2awpGwgwwb1r/fLG2OMr3kzEXQCdnosZ7jXVeeXwLyqNojIbSKyXESWZ2dn12GIwLavCcrfzftlIxnbv13dntsYYxqBBvHWkIj8HEgCnq5qu6pOV9UkVU2Kjo6u24unzCBfItgRPZKuUTbktDHG/3jzraFdgOcL+bHudccQkQuB3wEjVbXIi/EcrzAXXf8JH5WOYPSALvV6aWOMaSi8+USQDPQSkW4iEgJMBuZ47iAiCcBrwHhVzfJiLFVb9yFSWsh7pecxtp9VCxlj/JPXEoGqlgLTgPnAeuBdVU0VkSdEZLx7t6eBCOA9EUkRkTnVnM47UmawK7grOS36079ji3q9tDHGNBRe7VCmqnOBuZXWPerx+UJvXr9G+zZDxjLeLruOscPaIyI+C8UYY3ypQTQW+0TKDFwSyAclw7mov3UiM8b4L/8cYsJVBqtnsj58KKUSQ1LX1r6OyBhjfMY/nwi2LoK83bx+eDgXntGOIJuS0hjjx/yzBEyZQUlIKz4rjLdqIWOM3/O/RHDkAKz/lOWRFxAUEsa5vdr6OiJjjPEp/0sE6z6EsiJeyT2Tkb2jCQu2KSmNMf7N/xJBygyOtO7LkvxONraQMcbgb4kgeyPsWs53kWMJCgjg/D6WCIwxxr8SQco7qATy8r4hnNU9ipbhNiWlMcb4TyIoK4XVszjc9QJW7g/mIqsWMsYYwJ8SwdZFkL+XJc3HADDGpqQ0xhjAn3oWH9oNrbvx+t5exHcOoX3LMF9HZIwxDYL/PBEMuYHdv/iOlbsKrFrIGGM8+E8iAL5Y70xzOdaqhYwxpoJfJYL5qXvpEd2cnjERvg7FGGMaDL9JBAcLivlx237G2thCxhhzDL9JBAvXZ1HmUhtkzhhjKvGbRNCiWTBj+rVjUKeWvg7FGGMaFL95fXRMv3aMsQnqjTHmOH7zRGCMMaZqlgiMMcbPWSIwxhg/Z4nAGGP8nCUCY4zxc5YIjDHGz1kiMMYYP2eJwBhj/Jyoqq9jOCkikg38dIqHtwX21WE4jYF9Z/9g39k/nM537qqq0VVtaHSJ4HSIyHJVTfJ1HPXJvrN/sO/sH7z1na1qyBhj/JwlAmOM8XP+lgim+zoAH7Dv7B/sO/sHr3xnv2ojMMYYczx/eyIwxhhTiSUCY4zxc36TCERknIhsFJF0EXnI1/F4m4h0FpFFIpImIqkicq+vY6oPIhIoIqtE5FNfx1IfRKSViLwvIhtEZL2InO3rmLxNRO53/5teJyL/FZEwX8dU10TkDRHJEpF1HuvaiMgXIrLZ/bt1XV3PLxKBiAQCLwEXA/2AKSLSz7dReV0p8GtV7QecBdzlB98Z4F5gva+DqEfPA5+ral8gnib+3UWkE3APkKSqA4BAYLJvo/KKt4BxldY9BCxU1V7AQvdynfCLRAAMA9JVdauqFgMzgQk+jsmrVHWPqq50f87DKSA6+TYq7xKRWOBS4HVfx1IfRKQlcB7wTwBVLVbVg76Nql4EAc1EJAgIB3b7OJ46p6pLgP2VVk8A/uX+/C/gZ3V1PX9JBJ2AnR7LGTTxQtGTiMQBCcCPvo3E654DHgRcvg6knnQDsoE33dVhr4tIc18H5U2qugt4BtgB7AFyVXWBb6OqN+1UdY/7816gziZh95dE4LdEJAL4ALhPVQ/5Oh5vEZHLgCxVXeHrWOpREJAIvKKqCcBh6rC6oCFy14tPwEmCHYHmIvJz30ZV/9R577/O3v33l0SwC+jssRzrXtekiUgwThJ4R1U/9HU8XnYOMF5EtuNU/Z0vIv/xbUhelwFkqGr5k977OImhKbsQ2Kaq2apaAnwIDPdxTPUlU0Q6ALh/Z9XVif0lESQDvUSkm4iE4DQuzfFxTF4lIoJTd7xeVZ/1dTzepqoPq2qsqsbh/Pf9SlWb9J2iqu4FdopIH/eqC4A0H4ZUH3YAZ4lIuPvf+AU08QZyD3OAG9yfbwA+rqsTB9XViRoyVS0VkWnAfJy3DN5Q1VQfh+Vt5wDXA2tFJMW97hFVnevDmEzduxt4x32DsxW4ycfxeJWq/igi7wMrcd6MW0UTHGpCRP4LjALaikgG8BjwZ+BdEfklzlD8V9fZ9WyICWOM8W/+UjVkjDGmGpYIjDHGz1kiMMYYP2eJwBhj/JwlAmOM8XOWCEyDJSIqIn/1WH5ARP5QR+d+S0SuqotzneA6k9yjgi7y9rUqXfdGEfl7fV7TNF6WCExDVgRcISJtfR2IJ/dgZ7X1S+BWVR3trXiMOV2WCExDVorTWej+yhsq39GLSL779ygR+VpEPhaRrSLyZxG5TkSWichaEenhcZoLRWS5iGxyj1VUPp/B0yKSLCJrROR2j/N+IyJzqKL3rohMcZ9/nYj8xb3uUeBc4J8i8nQVx/zG4zqPu9fFuecWeMf9JPG+iIS7t13gHlxurXu8+lD3+qEislREVru/Z6T7Eh1F5HP3+PVPeXy/t9xxrhWR4/62xv/4Rc9i06i9BKwpL8hqKR44A2cY363A66o6TJzJee4G7nPvF4czRHkPYJGI9AR+gTOi5VB3QfudiJSPbpkIDFDVbZ4XE5GOwF+AIcABYIGI/ExVnxCR84EHVHV5pWPGAr3c1xdgjoichzOEQh/gl6r6nYi8AdzpruZ5C7hAVTeJyL+BO0TkZWAWcI2qJotIC+CI+zKDcUadLQI2isiLQAzQyT2WPyLS6iT+rqaJsicC06C5R0z9N85kJLWV7J6PoQjYApQX5GtxCv9y76qqS1U34ySMvsBY4BfuYTl+BKJwCmyAZZWTgNtQYLF7ILRS4B2ceQJqMtb9swpnuIS+HtfZqarfuT//B+epog/OYGub3Ov/5b5GH2CPqiaD8/dyxwDOJCa5qlqI8xTT1f09u4vIiyIyDmiyI9Ka2rMnAtMYPIdTWL7psa4U942MiAQAIR7bijw+uzyWXRz7b77y+CqKc3d+t6rO99wgIqNwhnmuKwL8SVVfq3SduGriOhWef4cyIEhVD4hIPHARMBVnvJqbT/H8pomwJwLT4KnqfuBdnIbXcttxqmIAxgPBp3DqSSIS4G436A5sxBmY8A73EN6ISO9aTPayDBgpIm3FmRZ1CvD1CY6ZD9zsni8CEekkIjHubV3k6NzD1wLfumOLc1dfgTOg4Nfu9R1EZKj7PJE1NWa7G94DVPUD4Pc0/WGrTS3YE4FpLP4KTPNY/gfwsYisBj7n1O7Wd+AU4i2AqapaKCKv41QfrXQPc5zNCaYEVNU9IvIQsAjnTv8zVa1xiGBVXSAiZwDfO5chH/g5zp37Rpw5pt/AqdJ5xR3bTcB77oI+GXhVVYtF5BrgRRFphtM+cGENl+6EM6NZ+U3gwzXFafyDjT5qTAPirhr6tLwx15j6YFVDxhjj5+yJwBhj/Jw9ERhjjJ+zRGCMMX7OEoExxvg5SwTGGOPnLBEYY4yf+382uRMzIS8YVwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkbUpRzz3CPB"
      },
      "source": [
        "# Optional Exercise\n",
        "\n",
        "Neural Network models include a lot of hyperparameters. These parameters are not learnt, but the choice for these are left to the designer/programmer. Model architecture is a hyperparameter. In this problem, we've provided you the model architecture - number of hidden layers, number of nodes in each hidden layer, choice of activation function, etc. Let's keep them fixed for now. We might explore tuning these in future.\n",
        "\n",
        "\n",
        "There is another set of hyper-parameters associated with the training process - learning rate of the gradient descent algorithm, learning rate decay, number of training epochs, batch size (this is an important one).\n",
        "Try tuning these hyperparameters. You can use validation accuracy to choose the values (Good hyperparameters should give higher validation accuracy).\n",
        "\n",
        "\n",
        "Evaluate your acurracy of your new trained model on test set, and compare with previous result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc1iZEqKeyK3"
      },
      "source": [
        "# Problem 1.2: Convolutional Neural Network\n",
        "In this problem you will train a network with the following architecture: conv - relu - 2x2 max pool - fc - relu - fc - softmax, and test it out on the MNIST dataset.\n",
        "\n",
        "The outputs of the last fully connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part.\n",
        "\n",
        "GREAT RESOURCE: https://cs231n.github.io/convolutional-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJTy3spfTjA"
      },
      "source": [
        "## Layers\n",
        "In this part, we implement the layers you need for your network.\n",
        "\n",
        "Hint - Think about efficient implementation here, you should never be required to loop over batch_size dimension. A naive implementation will take over 30 minutes to run, efficient implementation can reduce the runtime 10-fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8mlTosufPEV"
      },
      "source": [
        "def conv_forward(x, w):\n",
        "    \"\"\"\n",
        "    The input consists of N data points, each with C channels, height H and\n",
        "    width W. We filter each input with F different filters, where each filter\n",
        "    spans all C channels and has height HH and width WW.\n",
        "    You'll be implementing \"valid\" convolution here (no padding)\n",
        "    \n",
        "    Input:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - w: Filter weights of shape (F, C, HH, WW)\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
        "      H' = H - HH + 1\n",
        "      W' = W - WW + 1\n",
        "    - cache: (x, w)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional forward pass.                         #\n",
        "    ###########################################################################\n",
        "    # Extract shapes and constants\n",
        "    \n",
        "    (N, C, H, W) = x.shape\n",
        "    (F, C, HH, WW) = w.shape\n",
        "    H_prime = H - HH + 1\n",
        "    W_prime = W - WW + 1\n",
        "    x = x.reshape((N,1, C, H, W))\n",
        "    w = w.reshape((1, F, C, HH, WW))\n",
        "    out = np.zeros((N,F,H_prime, W_prime))\n",
        "    for h in range(H_prime):\n",
        "      for j in range(W_prime):\n",
        "        multiplication = np.multiply(x[:,:,:,h:h+HH,j:j+WW],w)\n",
        "        out[:,:, h, j] = np.sum(multiplication, axis = (2,3,4))\n",
        "    x = x.reshape((N,C,H,W))\n",
        "    w = w.reshape((F,C,HH,WW))\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w)\n",
        "    return out, cache\n",
        "\n",
        "def conv_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives.\n",
        "    - cache: A tuple of (x, w) as in conv_forward\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, shape (N, C, H, W)\n",
        "    - dw: Gradient with respect to w, shape (F, C, HH, WW)\n",
        "    \"\"\"\n",
        "    dx, dw = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional backward pass.                        #\n",
        "    ###########################################################################\n",
        "    (x, w) = cache\n",
        "    (N, C, H, W) = x.shape\n",
        "    (F, C, HH, WW) = w.shape\n",
        "    dx = np.zeros(x.shape)\n",
        "    dw = np.zeros(w.shape)\n",
        "    N, F, prev_H, prev_W = dout.shape\n",
        "    for h in range(prev_H):\n",
        "      for j in range(prev_W):\n",
        "        dx[:,:,h:h + HH, j:j + WW] += np.sum(w[None,:,:,:,:] * dout[:,:,h,j][:,:,None, None, None], axis = 1)\n",
        "        dw += np.sum(x[:,None,:,h:h + HH, j:j + WW] * dout[:,:,h,j][:,:,None, None, None], axis = 0)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw\n",
        "  \n",
        "\n",
        "def max_pool_forward(x, pool_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a max-pooling layer.\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C, H, W)\n",
        "    - pool_param: dictionary with the following keys:\n",
        "      - 'pool_height': The height of each pooling region\n",
        "      - 'pool_width': The width of each pooling region\n",
        "      - 'stride': The distance between adjacent pooling regions\n",
        "    No padding is necessary here. Output size is given by \n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n",
        "      H' = 1 + (H - pool_height) / stride\n",
        "      W' = 1 + (W - pool_width) / stride\n",
        "    - cache: (x, pool_param)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the max-pooling forward pass                            #\n",
        "    ###########################################################################\n",
        "    N, C, H, W = x.shape\n",
        "    pool_height = pool_param['pool_height']\n",
        "    pool_width = pool_param['pool_width']\n",
        "    stride = pool_param['stride']\n",
        "    H_prime = 1 + ((H - pool_height) // stride)\n",
        "    W_prime = 1 + ((W - pool_width) // stride)\n",
        "    out = np.zeros((N, C, H_prime, W_prime))\n",
        "    for h in range(H_prime):\n",
        "      for w in range(W_prime):\n",
        "        small_region = x[:, :, h * stride: h * stride + pool_height, w * stride: w * stride + pool_width]\n",
        "        out[:, :, h, w] = np.max(small_region, axis = (2,3))\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, pool_param)\n",
        "    return out, cache\n",
        "\n",
        "def max_pool_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a max-pooling layer.\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives\n",
        "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the max-pooling backward pass                           #\n",
        "    ###########################################################################\n",
        "    x, pool_param = cache\n",
        "    N,C,H,W = x.shape\n",
        "    stride = pool_param['stride']\n",
        "    pool_height = pool_param['pool_height']\n",
        "    pool_width = pool_param['pool_width']\n",
        "\n",
        "    H_prime = 1 + (H - pool_height) // stride\n",
        "    W_prime = 1 + (W - pool_width) // stride\n",
        "    \n",
        "    dx = np.zeros_like(x)\n",
        "    for h in range(H_prime):\n",
        "      for w in range(W_prime):\n",
        "          small_region_x = x[:, :, h * stride: h * stride + pool_height, w * stride: w * stride + pool_width]\n",
        "          max_values = np.max(small_region_x, axis=(2,3))\n",
        "          indicator_matrix = small_region_x == max_values\n",
        "          dx[:,:, h * stride: h * stride + pool_height, w * stride: w * stride + pool_width] += dout[:,:,h,w][:,:,None, None] * indicator_matrix\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "def fc_forward_CNN(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    \n",
        "    x_reshaped = x.reshape(x.shape[0], -1)\n",
        "    fc_output = np.dot(x_reshaped, w) + b\n",
        "    cache = (x, w, b)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    \n",
        "    return fc_output, cache\n",
        "\n",
        "def fc_backward_CNN(dout,cache):\n",
        "    x, w, b = cache\n",
        "    x_reshaped = x.reshape(x.shape[0], -1)\n",
        "    dx = np.dot(dout,w.T)\n",
        "    dx = dx.reshape(x.shape)\n",
        "    dw = np.dot(np.transpose(x_reshaped), dout)\n",
        "    db = np.sum(dout,0)\n",
        "    return dx, dw, db\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDZy7Mw6fiOx"
      },
      "source": [
        "#  CNN Classifier\n",
        "\n",
        "In this problem, implement ConvNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANN0nOfYfhPN"
      },
      "source": [
        "class ConvNet(object):\n",
        "  \"\"\"\n",
        "  A convolutional network with the following architecture:\n",
        "  \n",
        "  conv - relu - 2x2 max pool - fc - relu - fc - softmax \n",
        "  \n",
        "  The network operates on minibatches of data that have shape (N, C, H, W)\n",
        "  consisting of N images, each with height H and width W and with C input\n",
        "  channels.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, input_dim=(1, 28, 28), num_filters=32, filter_size=7,\n",
        "               hidden_dim=100, num_classes=10, weight_scale=1e-3, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    Initialize a new network.\n",
        "    \n",
        "    Inputs:\n",
        "    - input_dim: Tuple (N, C, H, W) giving size of input data\n",
        "    - num_filters: Number of filters to use in the convolutional layer\n",
        "    - filter_size: Size of filters to use in the convolutional layer\n",
        "    - hidden_dim: Number of nodes to use in the fully-connected hidden layer\n",
        "    - num_classes: Number of unique classification labels.\n",
        "    - weight_scale: Scalar giving standard deviation for random initialization\n",
        "      of weights.\n",
        "    - dtype: numpy datatype to use for computation.\n",
        "    \"\"\"\n",
        "    self.params = {}\n",
        "    self.dtype = dtype\n",
        "\n",
        "    \n",
        "    ############################################################################\n",
        "    # TODO: Initialize weights and biases for the three-layer convolutional    #\n",
        "    # network. Weights should be initialized from a Gaussian with standard     #\n",
        "    # deviation equal to weight_scale; biases should be initialized to zero.   #\n",
        "    # All weights and biases should be stored in the dictionary self.params.   #\n",
        "    # Store weights for the convolutional layer using the keys 'W1' (here      #\n",
        "    # we do not consider the bias term in the convolutional layer);            #\n",
        "    # use keys 'W2' and 'b2' for the weights and biases of the                 #\n",
        "    # hidden fully-connected layer, and keys 'W3' and 'b3' for the weights     #\n",
        "    # and biases of the output affine layer.                                   #\n",
        "    ############################################################################\n",
        "    W1 = np.random.normal(0, weight_scale, size=(num_filters, input_dim[0], filter_size, filter_size))\n",
        "    W2 = np.random.normal(0, weight_scale, size=(3872, hidden_dim))\n",
        "    b2 = np.zeros(hidden_dim)\n",
        "    W3 = np.random.normal(0, weight_scale, size=(hidden_dim, num_classes))\n",
        "    b3 = np.zeros(num_classes)\n",
        "\n",
        "\n",
        "    self.params['W1'] = W1\n",
        "    self.params['W2'] = W2\n",
        "    self.params['W3'] = W3\n",
        "    self.params['b2'] = b2\n",
        "    self.params['b3'] = b3\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    for k, v in self.params.items():\n",
        "      self.params[k] = v.astype(dtype)\n",
        "     \n",
        " \n",
        "  def forwards_backwards(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Evaluate loss and gradient for the three-layer convolutional network.\n",
        "    \n",
        "    Input / output: Same as forwards_backwards function in MLPClassifier above.\n",
        "    \"\"\"\n",
        "    W1 = self.params['W1']\n",
        "    W2, b2 = self.params['W2'], self.params['b2']\n",
        "    W3, b3 = self.params['W3'], self.params['b3']\n",
        "    \n",
        "    # pass pool_param to the forward pass for the max-pooling layer\n",
        "    pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
        "\n",
        "    scores = None\n",
        "    ############################################################################\n",
        "    # TODO: Implement the forward pass for the three-layer convolutional net,  #\n",
        "    # computing the class scores for X and storing them in the scores          #\n",
        "    # variable. You can use fc-layer and softmax_loss functions from before.\n",
        "\n",
        "    # Hint - In forwards pass, think about reshaping when passing output from \n",
        "    # max-pool layer to FC layer. In backwards pass, again think about reshaping\n",
        "    # the upstream gradient from fc-layer to max-pool layer.\n",
        "    ############################################################################\n",
        "    result_after_conv, cache1 = conv_forward(X, W1)\n",
        "    result_after_relu, cache2 = relu_forward(result_after_conv)\n",
        "    result_after_pooling, cache3 = max_pool_forward(result_after_relu, pool_param)\n",
        "    pool_shape = result_after_pooling.shape\n",
        "    result_after_pooling = result_after_pooling.reshape((result_after_pooling.shape[0], -1))\n",
        "    result_after_fc, cache4 = fc_forward_CNN(result_after_pooling, W2, b2)\n",
        "    relu_out, cache5 = relu_forward(result_after_fc)\n",
        "    scores,cache6 = fc_forward(relu_out, W3, b3)\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    \n",
        "    if y is None:\n",
        "      return scores\n",
        "    grads = {}\n",
        "    ############################################################################\n",
        "    # TODO: Implement the backward pass for the three-layer convolutional net, #\n",
        "    # storing the loss and gradients in the loss and grads variables. Compute  #\n",
        "    # loss using softmax_loss function implemented before, and make sure that \n",
        "    # grads[k] holds the gradients for self.params[k].                         #\n",
        "    ############################################################################\n",
        "    loss, dscores = softmax_loss(scores, y)\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    dh6, dw6, db6 = fc_backward_CNN(dscores, cache6)\n",
        "\n",
        "    dx5 = relu_backward(dh6, cache5)\n",
        "\n",
        "    # dh4 = np.dot(dx5, cache4[1].T)\n",
        "    # dw4 = np.einsum('ijkm,mkjh->ih', cache4[0].T, dx5)\n",
        "    # db4 = np.sum(dx5, axis=0)\n",
        "    dh4, dw4, db4 = fc_backward_CNN(dx5, cache4)\n",
        "    dh4 = dh4.reshape(pool_shape)\n",
        "    dx3 = max_pool_backward(dh4, cache3)\n",
        "    dx2 = relu_backward(dx3, cache2)\n",
        "    dx,dw = conv_backward(dx2, cache1)\n",
        "\n",
        "    grads['W3'] = dw6\n",
        "    grads['W2'] = dw4\n",
        "    grads['W1'] = dw\n",
        "    grads['b2'] = db4\n",
        "    grads['b3'] = db6\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    \n",
        "    return loss, grads"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ00sUc5SBDI"
      },
      "source": [
        "# Training and evaluation functions\n",
        "In the following section, we provide you helper functions to train your neural network model and evaluate your trained model on test/validation set. \n",
        "\n",
        "You are not required to implement anything here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSyMsfkyP6nx"
      },
      "source": [
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 200)\n",
        "    num_val_samples = kwargs.pop('num_val_samples',200)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    num_iterations = 60\n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "           \n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "   \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxIEeJ_BgEA-"
      },
      "source": [
        "## Train your model\n",
        "\n",
        "Here you'll load MNIST dataset from keras modules and train your ConvNet model.\n",
        "\n",
        "Run this cell to run training. You're not required to implement anything here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou1mepm_gghC",
        "outputId": "9d12201f-a525-4e98-9035-b42faa10a2cc"
      },
      "source": [
        "np.random.seed(100)\n",
        "# load data\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "train_data = { 'X_train':x_train[:,None,:,:], 'X_val': x_test[:,None,:,:], 'y_train': y_train, 'y_val': y_test}\n",
        "\n",
        "# initialize model\n",
        "model = ConvNet(hidden_dim =250, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 0.001,\n",
        "    lr_decay=.95, num_epochs=1, \n",
        "    batch_size=64, print_every=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:135: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 60) loss: -2.228580\n",
            "(Epoch 0 / 1) train acc: 0.125000; val_acc: 0.145000\n",
            "(Iteration 2 / 60) loss: -0.208589\n",
            "(Iteration 3 / 60) loss: 0.100517\n",
            "(Iteration 4 / 60) loss: 0.114356\n",
            "(Iteration 5 / 60) loss: 0.003654\n",
            "(Iteration 6 / 60) loss: 0.063125\n",
            "(Iteration 7 / 60) loss: 0.034307\n",
            "(Iteration 8 / 60) loss: -0.122917\n",
            "(Iteration 9 / 60) loss: -0.013779\n",
            "(Iteration 10 / 60) loss: 0.120061\n",
            "(Iteration 11 / 60) loss: 0.027898\n",
            "(Iteration 12 / 60) loss: 0.002820\n",
            "(Iteration 13 / 60) loss: -0.044402\n",
            "(Iteration 14 / 60) loss: 0.054101\n",
            "(Iteration 15 / 60) loss: -0.000169\n",
            "(Iteration 16 / 60) loss: 0.030289\n",
            "(Iteration 17 / 60) loss: -0.076589\n",
            "(Iteration 18 / 60) loss: 0.112569\n",
            "(Iteration 19 / 60) loss: -0.027746\n",
            "(Iteration 20 / 60) loss: -0.075704\n",
            "(Iteration 21 / 60) loss: -0.006154\n",
            "(Iteration 22 / 60) loss: -0.064588\n",
            "(Iteration 23 / 60) loss: 0.091008\n",
            "(Iteration 24 / 60) loss: -0.075525\n",
            "(Iteration 25 / 60) loss: -0.015772\n",
            "(Iteration 26 / 60) loss: 0.041046\n",
            "(Iteration 27 / 60) loss: -0.019360\n",
            "(Iteration 28 / 60) loss: 0.012073\n",
            "(Iteration 29 / 60) loss: -0.032519\n",
            "(Iteration 30 / 60) loss: -0.047149\n",
            "(Iteration 31 / 60) loss: -0.004914\n",
            "(Iteration 32 / 60) loss: 0.041527\n",
            "(Iteration 33 / 60) loss: 0.034588\n",
            "(Iteration 34 / 60) loss: -0.058928\n",
            "(Iteration 35 / 60) loss: 0.022045\n",
            "(Iteration 36 / 60) loss: 0.031290\n",
            "(Iteration 37 / 60) loss: 0.070118\n",
            "(Iteration 38 / 60) loss: 0.053545\n",
            "(Iteration 39 / 60) loss: -0.024294\n",
            "(Iteration 40 / 60) loss: -0.083083\n",
            "(Iteration 41 / 60) loss: -0.001519\n",
            "(Iteration 42 / 60) loss: -0.012925\n",
            "(Iteration 43 / 60) loss: 0.044842\n",
            "(Iteration 44 / 60) loss: 0.066572\n",
            "(Iteration 45 / 60) loss: -0.143029\n",
            "(Iteration 46 / 60) loss: 0.106282\n",
            "(Iteration 47 / 60) loss: -0.058083\n",
            "(Iteration 48 / 60) loss: 0.087329\n",
            "(Iteration 49 / 60) loss: -0.111541\n",
            "(Iteration 50 / 60) loss: 0.123508\n",
            "(Iteration 51 / 60) loss: -0.020820\n",
            "(Iteration 52 / 60) loss: 0.096013\n",
            "(Iteration 53 / 60) loss: -0.087037\n",
            "(Iteration 54 / 60) loss: 0.013380\n",
            "(Iteration 55 / 60) loss: 0.039255\n",
            "(Iteration 56 / 60) loss: 0.039382\n",
            "(Iteration 57 / 60) loss: 0.035530\n",
            "(Iteration 58 / 60) loss: -0.126482\n",
            "(Iteration 59 / 60) loss: 0.064048\n",
            "(Iteration 60 / 60) loss: 0.090368\n",
            "(Epoch 0 / 1) train acc: 0.825000; val_acc: 0.780000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F93-DsV2_3LN"
      },
      "source": [
        "# Report final Validation Accuracy\n",
        "\n",
        "Since we didn't define a separate test set here, we just ask you to report the final validation accuracy after training is complete.\n",
        "\n",
        "Run the cell below. If you implemented everything correctly, you should atleast 75% final validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ieggb5F5_-Wy",
        "outputId": "6b4df43b-8469-45d5-f27e-9f1b378ca93f"
      },
      "source": [
        "# report final validation accuracy\n",
        "print('Final Validation accuracy = ', val_acc_history[-1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Validation accuracy =  0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9RC30C_6FLJ"
      },
      "source": [
        "Optional Exercise - You may experiment with the ConvNet architecture used above by adding a dropout layer or batch normalization layer. \n",
        "\n",
        "You can read about batch normalization and dropout layers here - https://cs231n.github.io/neural-networks-2/#batchnorm\n",
        "\n",
        "Retrain your new ConvNet with MNIST data and compare final validation accuracies."
      ]
    }
  ]
}